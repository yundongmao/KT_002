{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdf\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "emoij_map = {}\n",
    "emoij_map.setdefault(\"Clap\",len(emoij_map))\n",
    "emoij_map.setdefault(\"Cry\",len(emoij_map))\n",
    "emoij_map.setdefault(\"Disappoint\",len(emoij_map))\n",
    "emoij_map.setdefault(\"Explode\",len(emoij_map))\n",
    "emoij_map.setdefault(\"FacePalm\",len(emoij_map))\n",
    "emoij_map.setdefault(\"Hands\",len(emoij_map))\n",
    "emoij_map.setdefault(\"Neutral\",len(emoij_map))\n",
    "emoij_map.setdefault(\"Shrug\",len(emoij_map))\n",
    "emoij_map.setdefault(\"Think\",len(emoij_map))\n",
    "emoij_map.setdefault(\"Upside\",len(emoij_map))\n",
    "print(\"asdf\")\n",
    "# Clap: üëè (U+1F44F)\n",
    "# Cry: üò≠ (U+1F62D)\n",
    "# Disappoint: üòû (U+1F61E)\n",
    "# Explode: ü§Ø (U+1F92F)\n",
    "# FacePalm: ü§¶ (U+1F926)\n",
    "# Hands: üôå (U+1F64C)\n",
    "# Neutral: üòê (U+1F610)\n",
    "# Shrug: ü§∑ (U+1F937)\n",
    "# Think: ü§î (U+1F914)\n",
    "# Upside: üôÉ (U+1F643)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37143\n",
      "12164\n",
      "12159\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "f = open(\"train_raw.txt\")\n",
    "train_tweets_raw = []\n",
    "while True:\n",
    "    temp = f.readline()\n",
    "    if temp:   \n",
    "        train_tweets_raw.append(temp)\n",
    "    else:\n",
    "        break\n",
    "print len(train_tweets_raw)\n",
    "f.close()\n",
    "\n",
    "f = open(\"dev_raw.txt\")\n",
    "dev_tweets_raw = []\n",
    "while True:\n",
    "    temp = f.readline()\n",
    "    if temp:   \n",
    "        dev_tweets_raw.append(temp)\n",
    "    else:\n",
    "        break\n",
    "print len(dev_tweets_raw)\n",
    "f.close()\n",
    "\n",
    "f = open(\"test_raw.txt\")\n",
    "test_tweets_raw = []\n",
    "while True:\n",
    "    temp = f.readline()\n",
    "    if temp:   \n",
    "        test_tweets_raw.append(temp)\n",
    "    else:\n",
    "        break\n",
    "print(len(test_tweets_raw))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12159\n",
      "12164\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def myLemmatize(lemmatizer,tWord):\n",
    "    temp = lemmatizer.lemmatize(tWord)\n",
    "    if len(temp) < len(tWord):\n",
    "        return temp\n",
    "    return lemmatizer.lemmatize(tWord,\"v\")\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "word_number = {}\n",
    "number_word = {}\n",
    "\n",
    "def handleRawData(tweets_raw,isTest,word_number):\n",
    "    tweets_index = []\n",
    "    tweets_index_emoij = []\n",
    "    for tweet in tweets_raw:\n",
    "#         temp = tweet.split()\n",
    "        tweet = tweet.strip()\n",
    "        temp = re.split(\" |,|\\?|\\!|\\.|\\\\t\",tweet)\n",
    "#         print(temp)\n",
    "        if not isTest:\n",
    "            tweets_index_emoij.append(emoij_map[temp[1]])\n",
    "        temp_tweets_index = []\n",
    "        for s in temp[2:]:\n",
    "#             print s\n",
    "            if len(s) == 0:\n",
    "                continue\n",
    "            elif s[0] ==\"@\":\n",
    "                continue\n",
    "            elif s[0] == \"#\":\n",
    "                continue\n",
    "            elif s == \"RT\":\n",
    "                continue\n",
    "            elif s[:min(len(\"https://\"),len(s))] == \"https://\":\n",
    "                continue\n",
    "            elif s[:min(len(\"http://\"),len(s))] == \"http://\":\n",
    "                continue\n",
    "            elif not s.isalpha():\n",
    "                continue\n",
    "            tempword = myLemmatize(lemmatizer,s.lower())\n",
    "            if tempword in stopWords:\n",
    "                continue\n",
    "            wi = word_number.setdefault(tempword,len(word_number))\n",
    "            temp_tweets_index.append(wi)\n",
    "        tweets_index.append(temp_tweets_index)\n",
    "    return tweets_index,tweets_index_emoij\n",
    "\n",
    "train_tweets_index,train_tweets_index_emoij = handleRawData(train_tweets_raw,False,word_number)\n",
    "dev_tweets_index,dev_tweets_index_emoij = handleRawData(dev_tweets_raw,False,word_number)\n",
    "test_tweets_index,test_tweets_index_emoij = handleRawData(test_tweets_raw,True,word_number)\n",
    "for word in word_number:\n",
    "    number_word[word_number[word]] = word\n",
    "print(len(test_tweets_index))\n",
    "print(len(dev_tweets_index_emoij))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like,\n"
     ]
    }
   ],
   "source": [
    "print(myLemmatize(lemmatizer,\"like,\".lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def turnToSpaseMatrix(ll,feaNum):\n",
    "    row,column = [],[]\n",
    "    for i,l in enumerate(ll):\n",
    "        temp_index = set()\n",
    "        for index in l:\n",
    "            if index not in temp_index:\n",
    "                temp_index.add(index)\n",
    "                column.append(index)\n",
    "        row.extend([i]*len(temp_index))\n",
    "    data = [1]*len(row)\n",
    "    return coo_matrix((data,(row,column)),shape=(len(ll),feaNum))\n",
    "\n",
    "train_matrix = turnToSpaseMatrix(train_tweets_index,len(word_number))\n",
    "dev_matrix = turnToSpaseMatrix(dev_tweets_index,len(word_number))\n",
    "test_matrix = turnToSpaseMatrix(test_tweets_index,len(word_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.530828674778\n",
      "0.570618217692\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "bayesN = MultinomialNB(alpha=0.7, class_prior=None)\n",
    "bayesN.fit(train_matrix,np.array(train_tweets_index_emoij))\n",
    "result =  bayesN.predict(dev_matrix)\n",
    "count = 0\n",
    "for j in range(len(dev_tweets_index_emoij)):\n",
    "    if result[j] == dev_tweets_index_emoij[j]:\n",
    "        count+=1\n",
    "accuracy = count / float(len(dev_tweets_index_emoij))\n",
    "print(accuracy)\n",
    "\n",
    "\n",
    "linearL = LogisticRegression(penalty=\"l2\",solver=\"liblinear\",intercept_scaling=0.0625,max_iter=100,C=1.1)\n",
    "linearL.fit(train_matrix,np.array(train_tweets_index_emoij))\n",
    "result =  linearL.predict(dev_matrix)\n",
    "count = 0\n",
    "for j in range(len(dev_tweets_index_emoij)):\n",
    "    if result[j] == dev_tweets_index_emoij[j]:\n",
    "        count+=1\n",
    "accuracy = count / float(len(dev_tweets_index_emoij))\n",
    "print(accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56387701414\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# DecisionTreeClassifier()\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(train_matrix,np.array(train_tweets_index_emoij))\n",
    "result =  model.predict(dev_matrix)\n",
    "count = 0\n",
    "for j in range(len(dev_tweets_index_emoij)):\n",
    "    if result[j] == dev_tweets_index_emoij[j]:\n",
    "        count+=1\n",
    "accuracy = count / float(len(dev_tweets_index_emoij))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.575797435054\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_matrix,np.array(train_tweets_index_emoij))\n",
    "result =  model.predict(dev_matrix)\n",
    "count = 0\n",
    "for j in range(len(dev_tweets_index_emoij)):\n",
    "    if result[j] == dev_tweets_index_emoij[j]:\n",
    "        count+=1\n",
    "accuracy = count / float(len(dev_tweets_index_emoij))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[243, 18375, 0, 18, 12332, 46]\n",
      "31\t?\t@MichaelProOG @Phillips @DougKyed @nicolecyang @SHAQ Because, like Santa, Shaq is a sensitive big guy. \n",
      "\n",
      "16044\n"
     ]
    }
   ],
   "source": [
    "# print test_tweets_index[0]\n",
    "# print test_tweets_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11', 'Think', 'RT', '@InbetweenerVids:', 'Is', 'that', 'David', 'Hayes', 'promoter', 'or', 'Will', 'McKenzie', 'at', 'the', '#BellewHaye', 'press', 'conference', 'today', '@EddieHearn', '@TonyBellew', 'ht']\n",
      "Upside\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print train_tweets_raw[0].split()\n",
    "# print train_tweets_raw[100].split()[1]\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
